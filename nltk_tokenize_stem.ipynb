{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "bsuWiR8-P0b9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "Z4M83URhQEhi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()"
      ],
      "metadata": {
        "id": "SFlnDGl0QV8p"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer.stem('Understanding')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5gHy20Q6Qady",
        "outputId": "2eba294d-44d7-43c9-87e8-360bce908a94"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'understand'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=['eat','ingeating','history','wtite','writing','writes']"
      ],
      "metadata": {
        "id": "QdrFVVZjQdUZ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"----->\"+stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftkRz7B7QqFp",
        "outputId": "4a1ed4ce-8b17-451d-cfb7-3efa03fbc0e0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eat----->eat\n",
            "ingeating----->ingeat\n",
            "history----->histori\n",
            "wtite----->wtite\n",
            "writing----->write\n",
            "writes----->write\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "Vm3WhX6VQzZW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg=RegexpStemmer('ing|s$',min=4)"
      ],
      "metadata": {
        "id": "2G1r-bZSQ61q"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"----->\"+reg.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbD5tYklRHZE",
        "outputId": "a55663cb-8cc3-4183-a854-d44c20f9538d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eat----->eat\n",
            "ingeating----->eat\n",
            "history----->history\n",
            "wtite----->wtite\n",
            "writing----->writ\n",
            "writes----->write\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "aWCj3jetRfbK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemm=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "_GK-09TgSEl9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemm.lemmatize(\"churches\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h9hgmeNvSR8D",
        "outputId": "c0787786-ff60-42d2-812b-d528926a57c9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'church'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"----->\"+lemm.lemmatize(word,pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryOWzU9XUuQF",
        "outputId": "3ebef9df-74b8-49d8-8658-64a5ea6c8166"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eat----->eat\n",
            "ingeating----->ingeating\n",
            "history----->history\n",
            "wtite----->wtite\n",
            "writing----->write\n",
            "writes----->write\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "GWqSoxafUlB7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JDYMF6BNUm-E"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfQ8YEbUUiss",
        "outputId": "f791db0b-343e-46cc-92b6-71c256710fbd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "WjbZbvfmSXeH"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "para=\"\"\"Colab is a hosted @ Jupyter Notebook service that '\\n' requires no, setup to use and provides\n",
        "free access to computing resources, including GPUs and TPUs. Colab is especially. well suited to machine\n",
        " learning, data science, and education.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hPjk9pY5TJ3t"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "para"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "NdDMja5qTSNS",
        "outputId": "d1c6cf18-23ef-4d86-8460-aba94675a0df"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Colab is a hosted @ Jupyter Notebook service that '\\n' requires no, setup to use and provides\\nfree access to computing resources, including GPUs and TPUs. Colab is especially. well suited to machine\\n learning, data science, and education.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(para)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDsg6irpTcTU",
        "outputId": "e13edb6d-b6a6-46cc-859a-f6355dfda816"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Colab is a hosted @ Jupyter Notebook service that '\\n' requires no, setup to use and provides\\nfree access to computing resources, including GPUs and TPUs.\",\n",
              " 'Colab is especially.',\n",
              " 'well suited to machine\\n learning, data science, and education.']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RVVJzsHUZ1P",
        "outputId": "ee63e19a-759d-440d-bbe4-7e2261b15feb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "r5dFjjt4VQXa"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(para)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZSZ12HmV5iM",
        "outputId": "60c925b1-3e07-4795-8197-acac65bd8801"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Colab',\n",
              " 'is',\n",
              " 'a',\n",
              " 'hosted',\n",
              " '@',\n",
              " 'Jupyter',\n",
              " 'Notebook',\n",
              " 'service',\n",
              " 'that',\n",
              " \"'\",\n",
              " \"'\",\n",
              " 'requires',\n",
              " 'no',\n",
              " ',',\n",
              " 'setup',\n",
              " 'to',\n",
              " 'use',\n",
              " 'and',\n",
              " 'provides',\n",
              " 'free',\n",
              " 'access',\n",
              " 'to',\n",
              " 'computing',\n",
              " 'resources',\n",
              " ',',\n",
              " 'including',\n",
              " 'GPUs',\n",
              " 'and',\n",
              " 'TPUs',\n",
              " '.',\n",
              " 'Colab',\n",
              " 'is',\n",
              " 'especially',\n",
              " '.',\n",
              " 'well',\n",
              " 'suited',\n",
              " 'to',\n",
              " 'machine',\n",
              " 'learning',\n",
              " ',',\n",
              " 'data',\n",
              " 'science',\n",
              " ',',\n",
              " 'and',\n",
              " 'education',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "para.split(\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIcp-VmlV8EP",
        "outputId": "4179f3cd-724f-472c-d246-85d77b9aaf4f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Colab',\n",
              " 'is',\n",
              " 'a',\n",
              " 'hosted',\n",
              " '@',\n",
              " 'Jupyter',\n",
              " 'Notebook',\n",
              " 'service',\n",
              " 'that',\n",
              " \"'\\n'\",\n",
              " 'requires',\n",
              " 'no,',\n",
              " 'setup',\n",
              " 'to',\n",
              " 'use',\n",
              " 'and',\n",
              " 'provides\\nfree',\n",
              " 'access',\n",
              " 'to',\n",
              " 'computing',\n",
              " 'resources,',\n",
              " 'including',\n",
              " 'GPUs',\n",
              " 'and',\n",
              " 'TPUs.',\n",
              " 'Colab',\n",
              " 'is',\n",
              " 'especially.',\n",
              " 'well',\n",
              " 'suited',\n",
              " 'to',\n",
              " 'machine\\n',\n",
              " 'learning,',\n",
              " 'data',\n",
              " 'science,',\n",
              " 'and',\n",
              " 'education.\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paragraph=\"\"\"A paragraph is a collection of words strung together to make a longer unit than a sentence.\n",
        " Several sentences often make a paragraph. There are normally three to eight sentences in a paragraph.\n",
        "  Paragraphs can start with a five-space indentation or by skipping a line and then starting over.\n",
        "   This makes it simpler to tell when one paragraph ends and the next starts simply it has 3-9 lines.\n",
        "\n",
        "A topic phrase appears in most ordered types of writing, such as essays.\n",
        "This paragraph's topic sentence informs the reader about the topic of the paragraph.\n",
        "In most essays, numerous paragraphs make statements to support a thesis statement, which is the essay's fundamental point.\n",
        "\n",
        "Paragraphs may signal when the writer changes topics. Each paragraph may have a number of sentences, depending on the topic.\"\"\""
      ],
      "metadata": {
        "id": "ohG4uYw3WFGT"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srRvd0gUDNfq",
        "outputId": "7dbf02e3-ace6-46c9-b18e-c411edecda6b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "bVOyccYrDY9g"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Dr6FE5KDemK",
        "outputId": "db920235-70ed-4245-9c50-7fe5a5ced5c5"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7YWC_HdD6lh",
        "outputId": "9113317c-cee3-4dd8-9fba-21a3b5ba6321"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A paragraph is a collection of words strung together to make a longer unit than a sentence.',\n",
              " 'Several sentences often make a paragraph.',\n",
              " 'There are normally three to eight sentences in a paragraph.',\n",
              " 'Paragraphs can start with a five-space indentation or by skipping a line and then starting over.',\n",
              " 'This makes it simpler to tell when one paragraph ends and the next starts simply it has 3-9 lines.',\n",
              " 'A topic phrase appears in most ordered types of writing, such as essays.',\n",
              " \"This paragraph's topic sentence informs the reader about the topic of the paragraph.\",\n",
              " \"In most essays, numerous paragraphs make statements to support a thesis statement, which is the essay's fundamental point.\",\n",
              " 'Paragraphs may signal when the writer changes topics.',\n",
              " 'Each paragraph may have a number of sentences, depending on the topic.']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PToU2VEOD8TD",
        "outputId": "1ee0168b-b03b-40ec-9802-1a0779fbdfbf"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatize=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "skMZW-9AESYD"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## stops words\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nkAr-aNEmAQ",
        "outputId": "8d96077e-29ea-4fec-9735-5edc3f2b81c2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrAaKTIMFM5_",
        "outputId": "ce61c532-00b9-4145-d712-0d507ab06081"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## now we will do word tokenization and then we will apply lemmatization to only those words which are not stopwords\n",
        "for i in range(len(sentences)):\n",
        "  words=word_tokenize(sentences[i])\n",
        "  stem_word=[word for word in words if word not in set(stop_words)]\n",
        "  sentences[i]=' '.join(stem_word)\n"
      ],
      "metadata": {
        "id": "fVmIBgh7FVQi"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIGGVg7vG8DK",
        "outputId": "556604fa-ccab-4cad-862a-7fe45f80cf12"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A paragraph collection words strung together make longer unit sentence .',\n",
              " 'Several sentences often make paragraph .',\n",
              " 'There normally three eight sentences paragraph .',\n",
              " 'Paragraphs start five-space indentation skipping line starting .',\n",
              " 'This makes simpler tell one paragraph ends next starts simply 3-9 lines .',\n",
              " 'A topic phrase appears ordered types writing , essays .',\n",
              " \"This paragraph 's topic sentence informs reader topic paragraph .\",\n",
              " \"In essays , numerous paragraphs make statements support thesis statement , essay 's fundamental point .\",\n",
              " 'Paragraphs may signal writer changes topics .',\n",
              " 'Each paragraph may number sentences , depending topic .']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7QX8gWfRHhR_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}